Com certeza. Eu analisei os arquivos do seu projeto, especialmente o `ETL.ipynb` e o `etl/extract.py`, para extrair os resultados e metodologias corretos.

Atualizei a fonte de dados (estava usando um link do Kaggle diferente do que o script `extract.py` utiliza), preenchi a se√ß√£o de "Principais Descobertas" com os dados do seu notebook (sa√≠das das c√©lulas 12 e 13) e atualizei a lista de tecnologias para incluir PySpark e Kaggle API, que s√£o centrais para o projeto.

Tamb√©m adicionei as instru√ß√µes para configurar a API do Kaggle, conforme voc√™ solicitou.

Aqui est√° a vers√£o completa e atualizada do seu `readme.md`:

-----

# üìä Decifrando o Profissional de Dados: Uma An√°lise de Vagas no LinkedIn

## üéØ 1. Objetivo do Projeto

Este projeto foi desenvolvido durante o **carrers in data** com o objetivo principal de analisar o mercado de trabalho para profissionais de dados (Data Scientists, Data Analysts, Data Engineers, etc.).

Utilizando um conjunto de dados de vagas publicadas no LinkedIn, buscamos identificar e dissecar quais s√£o as **caracter√≠sticas, habilidades t√©cnicas (hard skills) e habilidades comportamentais (soft skills)** mais esperadas e valorizadas pelas empresas atualmente.

## ‚ùì 2. O Problema

O campo de dados est√° em constante e r√°pida evolu√ß√£o. Para aspirantes que desejam entrar na √°rea e para profissionais que buscam se manter atualizados, surge a pergunta central:

> **"Quais s√£o as compet√™ncias essenciais que definem um profissional de dados competitivo no mercado atual?"**

Este projeto visa responder a essa pergunta por meio de uma an√°lise quantitativa e qualitativa das descri√ß√µes de vagas reais.

## üíæ 3. Fonte de Dados

A an√°lise √© baseada no conjunto de dados p√∫blico **"LinkedIn Job Postings Dataset"**, dispon√≠vel na plataforma Kaggle. Este dataset foi extra√≠do pelo script `etl/extract.py`.

  * **Link do Dataset:** [Kaggle - LinkedIn Job Postings Dataset](https://www.kaggle.com/datasets/arshkon/linkedin-job-postings)

Este dataset agrega milhares de postagens de vagas, contendo informa√ß√µes valiosas como:

  * T√≠tulo do Cargo (Job Title)
  * Descri√ß√£o da Vaga (Job Description)
  * Empresa (Company)
  * Localiza√ß√£o (Location)
  * N√≠vel de Senioridade (Seniority Level)
  * E outras informa√ß√µes relevantes.

## üõ†Ô∏è 4. Metodologia e An√°lise

Nossa abordagem para extrair insights dos dados, implementada no notebook `ETL.ipynb` usando PySpark, seguiu as seguintes etapas:

1.  **Extra√ß√£o (Extract):**

      * Download e extra√ß√£o autom√°tica dos dados do Kaggle utilizando a API oficial (conforme `etl/extract.py`).

2.  **Pr√©-processamento e Limpeza (Transform):**

      * Carregamento dos dados em um DataFrame Spark.
      * Tratamento de valores ausentes (NaN) e duplicados.
      * Corre√ß√£o e convers√£o (cast) dos tipos de dados (ex: sal√°rios para `double`, datas para `timestamp`).

3.  **An√°lise Explorat√≥ria de Dados (EDA) & Extra√ß√£o de Habilidades:**

      * An√°lise de vagas por n√≠vel de senioridade, tipo de contrato e modalidade (remoto vs. presencial).
      * **Mapeamento de Hard Skills:** Defini√ß√£o de uma lista-cat√°logo de tecnologias (Python, SQL, Spark, etc.). A descri√ß√£o de cada vaga foi tokenizada e cruzada com essa lista para identificar as tecnologias mencionadas.
      * **Mapeamento de Soft Skills:** Extra√ß√£o e contagem das habilidades listadas na coluna `skills_desc`.

4.  **Visualiza√ß√£o e Carga (Load):**

      * Agrega√ß√£o dos dados para sumarizar as descobertas.
      * Cria√ß√£o de um DataFrame final e exporta√ß√£o para `transformed_postings.csv` para consumo futuro (ex: dashboards).

## üìà 5. Principais Descobertas (Resultados)

A an√°lise realizada no notebook `ETL.ipynb` revelou os seguintes insights:

  * **Quais s√£o as 10 Hard Skills (tecnologias) mais demandadas?**
    *Nota: Com base na contagem de vagas em que a tecnologia foi mencionada (extra√≠do da C√©lula 13 do notebook).*

    1.  **R** (5.210 vagas)
    2.  **SQL** (5.181 vagas)
    3.  **Python** (4.647 vagas)
    4.  **Spark** (856 vagas)
    5.  **Hadoop** (313 vagas)
    6.  **PyTorch** (243 vagas)
    7.  **TensorFlow** (224 vagas)
    8.  **BigQuery** (101 vagas)
    9.  **Pandas** (95 vagas)
    10. **NumPy** (85 vagas)

  * **Qual a propor√ß√£o de vagas que exigem "Python" vs. "R"?**

      * Os dados mostraram uma leve prefer√™ncia por **R (5.210 vagas)** em compara√ß√£o com **Python (4.647 vagas)** no dataset analisado.

  * **Quais as Soft Skills (ou habilidades gerais) mais citadas?**
    *Nota: Com base na contagem da coluna `skills_desc` (extra√≠do da C√©lula 12 do notebook). A alta frequ√™ncia de termos de sa√∫de sugere que o dataset original √© amplo e n√£o se limita a vagas de tecnologia.*

    1.  People Skills (74 men√ß√µes)
    2.  Healthcare (65 men√ß√µes)
    3.  Hospice Care (60 men√ß√µes)
    4.  Patient Care (56 men√ß√µes)
    5.  Fundraising (53 men√ß√µes)
    6.  Verbal / Written Communication (53 men√ß√µes)

## üöÄ 6. Tecnologias Utilizadas

Este projeto foi constru√≠do utilizando as seguintes ferramentas e bibliotecas:

## üìÇ 7. Como Executar o Projeto

Para replicar esta an√°lise, siga os passos abaixo:

1.  **Clone o reposit√≥rio:**

    ```bash
    git clone [URL-DO-SEU-REPOSIT√ìRIO]
    cd [NOME-DO-REPOSIT√ìRIO]
    ```

2.  **Configure a API do Kaggle:**

      * Fa√ßa login no [Kaggle](https://www.kaggle.com).
      * V√° at√© seu perfil, clique em "Account" e na se√ß√£o "API", clique em "Create New API Token".
      * Isso far√° o download do arquivo `kaggle.json`.
      * Coloque este arquivo no local esperado pela API.
          * **Linux/Mac:** `~/.kaggle/kaggle.json`
          * **Windows:** `C:\Users\<SeuUsuario>\.kaggle\kaggle.json`
      * (Certifique-se de definir as permiss√µes corretas para o arquivo, se necess√°rio: `chmod 600 ~/.kaggle/kaggle.json`)

3.  **Crie e ative um ambiente virtual:**

    ```bash
    python -m venv venv
    source venv/bin/activate  # Para Linux/Mac
    .\venv\Scripts\activate   # Para Windows
    ```

4.  **Instale as depend√™ncias:**
    *O projeto depende de `pyspark`, `pandas`, `pyarrow` e `kaggle`.*

    ```bash
    pip install pyspark pandas pyarrow kaggle
    ```

    *(Opcional: crie um arquivo `requirements.txt` com essas depend√™ncias e rode `pip install -r requirements.txt`)*

5.  **Execute o script de Extra√ß√£o (ETL):**
    *Este script baixar√° os dados do Kaggle para a pasta `data/`.*

    ```bash
    python etl/extract.py
    ```

6.  **Execute os Notebooks:**
    Abra o Jupyter Lab (ou VS Code) e execute o notebook `ETL.ipynb` para processar os dados e gerar o arquivo `transformed_postings.csv`.

    ```bash
    jupyter lab
    ```

## üë• 8. Equipe

  * Deivyson Gomes
  * Davi Silva
  * Gustavo Henrique
  * Gabriel Estrela
  * Ant√¥nio Ramalho